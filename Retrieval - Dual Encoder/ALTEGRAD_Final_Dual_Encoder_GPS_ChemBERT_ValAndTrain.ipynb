{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f6f78c42",
      "metadata": {
        "id": "f6f78c42"
      },
      "source": [
        "# **Running the baseline**\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raphaelrubrice/BornToOverfit/blob/raph/baseline.ipynb\"> <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> </a>  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d44c459",
      "metadata": {
        "id": "8d44c459"
      },
      "source": [
        "## **Colab setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9a3c77b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9a3c77b",
        "outputId": "e0106c3a-bb61-48ea-f510-37d513698b07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# to avoid having the data on your drive\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/raphaelrubrice/BornToOverfit.git\n",
        "%cd BornToOverfit\n",
        "!git checkout Camille"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yFJ-AUT6w5g",
        "outputId": "6af43f70-539f-4b36-b274-480882881cd2"
      },
      "id": "9yFJ-AUT6w5g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BornToOverfit'...\n",
            "remote: Enumerating objects: 766, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 766 (delta 56), reused 42 (delta 28), pack-reused 674 (from 1)\u001b[K\n",
            "Receiving objects: 100% (766/766), 1.08 MiB | 25.72 MiB/s, done.\n",
            "Resolving deltas: 100% (521/521), done.\n",
            "/content/BornToOverfit\n",
            "Branch 'Camille' set up to track remote branch 'Camille' from 'origin'.\n",
            "Switched to a new branch 'Camille'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3346ea9",
      "metadata": {
        "id": "b3346ea9"
      },
      "source": [
        "Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d067adf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d067adf",
        "outputId": "c2e750fd-0837-4b08-b315-95532f32c6da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r data_baseline/requirements.txt (line 1)) (2.9.0+cu126)\n",
            "Collecting torch-geometric>=2.3.0 (from -r data_baseline/requirements.txt (line 2))\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from -r data_baseline/requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.12/dist-packages (from -r data_baseline/requirements.txt (line 4)) (4.57.3)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from -r data_baseline/requirements.txt (line 5)) (4.67.1)\n",
            "Collecting rdkit>=2023.3.1 (from -r data_baseline/requirements.txt (line 6))\n",
            "  Downloading rdkit-2025.9.3-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: nltk>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from -r data_baseline/requirements.txt (line 7)) (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric>=2.3.0->-r data_baseline/requirements.txt (line 2)) (3.13.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric>=2.3.0->-r data_baseline/requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric>=2.3.0->-r data_baseline/requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric>=2.3.0->-r data_baseline/requirements.txt (line 2)) (3.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric>=2.3.0->-r data_baseline/requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric>=2.3.0->-r data_baseline/requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->-r data_baseline/requirements.txt (line 3)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->-r data_baseline/requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->-r data_baseline/requirements.txt (line 3)) (2025.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r data_baseline/requirements.txt (line 4)) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r data_baseline/requirements.txt (line 4)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r data_baseline/requirements.txt (line 4)) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r data_baseline/requirements.txt (line 4)) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r data_baseline/requirements.txt (line 4)) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r data_baseline/requirements.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit>=2023.3.1->-r data_baseline/requirements.txt (line 6)) (11.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.8.0->-r data_baseline/requirements.txt (line 7)) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.8.0->-r data_baseline/requirements.txt (line 7)) (1.5.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.30.0->-r data_baseline/requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->-r data_baseline/requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric>=2.3.0->-r data_baseline/requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric>=2.3.0->-r data_baseline/requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric>=2.3.0->-r data_baseline/requirements.txt (line 2)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric>=2.3.0->-r data_baseline/requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric>=2.3.0->-r data_baseline/requirements.txt (line 2)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric>=2.3.0->-r data_baseline/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric>=2.3.0->-r data_baseline/requirements.txt (line 2)) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->-r data_baseline/requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric>=2.3.0->-r data_baseline/requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric>=2.3.0->-r data_baseline/requirements.txt (line 2)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric>=2.3.0->-r data_baseline/requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric>=2.3.0->-r data_baseline/requirements.txt (line 2)) (2026.1.4)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdkit-2025.9.3-cp312-cp312-manylinux_2_28_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit, torch-geometric\n",
            "Successfully installed rdkit-2025.9.3 torch-geometric-2.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r data_baseline/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8ad188a",
      "metadata": {
        "id": "b8ad188a"
      },
      "source": [
        "Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc9c409a",
      "metadata": {
        "id": "bc9c409a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42521540-65c1-48cc-afa7-70a27e45dafd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot open '/content/drive/MyDrive/Kaggle_ALTEGRAD/data/results/baseline.gsheet' for reading: Operation not supported\n"
          ]
        }
      ],
      "source": [
        "!cp -r /content/drive/MyDrive/Kaggle_ALTEGRAD/data /content/BornToOverfit/data_baseline/."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Configuration des chemins\n",
        "DATA_DIR = Path(\"data_baseline/data\")\n",
        "\n",
        "TRAIN_FILE = DATA_DIR / \"train_embeddings_RealChemBERT.pt\"\n",
        "VAL_FILE = DATA_DIR / \"validation_embeddings_RealChemBERT.pt\"\n",
        "\n",
        "\n",
        "def load_embeddings_pt(filepath):\n",
        "    print(f\"ğŸ“¥ Chargement de {filepath}...\")\n",
        "\n",
        "    data = torch.load(filepath, map_location='cpu')\n",
        "\n",
        "    # On rÃ©cupÃ¨re les clÃ©s qu'on a dÃ©finies lors de la gÃ©nÃ©ration\n",
        "    ids = data['ids']\n",
        "    embeddings_tensor = data['embeddings']\n",
        "\n",
        "    print(f\"âœ… ChargÃ© ! Shape: {embeddings_tensor.shape}\")\n",
        "    return ids, embeddings_tensor\n",
        "\n",
        "# 2. Logique Principale\n",
        "if TRAIN_FILE.exists() and VAL_FILE.exists():\n",
        "    print(\"ğŸš€ Fichiers .pt trouvÃ©s ! Chargement direct...\")\n",
        "\n",
        "    train_ids, train_embeddings = load_embeddings_pt(TRAIN_FILE)\n",
        "    val_ids, val_embeddings = load_embeddings_pt(VAL_FILE)\n",
        "\n",
        "    print(f\"Premier ID train : {train_ids[0]}\")\n",
        "    print(f\"Norme du premier vecteur : {train_embeddings[0].norm().item():.4f}\")\n",
        "\n",
        "else:\n",
        "    print(f\" Fichiers non trouvÃ©s : {TRAIN_FILE}\")\n",
        "    print(\"âš™ï¸ Lancement du script de gÃ©nÃ©ration...\")\n",
        "\n",
        "    cmd = [\n",
        "        \"python\", \"data_baseline/generate_description_embeddings_automatic.py\",\n",
        "        \"--data_dir\", \"data_baseline/data\",\n",
        "        \"--model_name\", \"recobo/chemical-bert-uncased\",\n",
        "        \"--pooling\", \"cls\",\n",
        "        \"--splits\", \"train\", \"validation\"\n",
        "    ]\n",
        "\n",
        "    process = subprocess.run(cmd, capture_output=False)\n",
        "\n",
        "    if process.returncode == 0:\n",
        "        print(\"\\n GÃ©nÃ©ration terminÃ©e avec succÃ¨s !\")\n",
        "    else:\n",
        "        print(\"\\n Erreur lors de la gÃ©nÃ©ration.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt0-2wnPiRLO",
        "outputId": "1976bc32-d82e-4fbc-ccdf-bdc2cf433f66"
      },
      "id": "yt0-2wnPiRLO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Fichiers .pt trouvÃ©s ! Chargement direct...\n",
            "ğŸ“¥ Chargement de data_baseline/data/train_embeddings_RealChemBERT.pt...\n",
            "âœ… ChargÃ© ! Shape: torch.Size([31008, 768])\n",
            "ğŸ“¥ Chargement de data_baseline/data/validation_embeddings_RealChemBERT.pt...\n",
            "âœ… ChargÃ© ! Shape: torch.Size([1000, 768])\n",
            "Premier ID train : 0\n",
            "Norme du premier vecteur : 24.6847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dual Encoder**"
      ],
      "metadata": {
        "id": "E9zVABzjuO5J"
      },
      "id": "E9zVABzjuO5J"
    },
    {
      "cell_type": "code",
      "source": [
        "# Avec backup automatique sur Drive (recommandÃ©)\n",
        "!python data_baseline/train_dual_encoder_optimized.py \\\n",
        "    --data_dir data_baseline/data \\\n",
        "    --drive_backup_dir /content/drive/MyDrive/Altegrad_Results/dual_encoder_checkpoints \\\n",
        "    --lr_gnn 8e-4 \\\n",
        "    --lr_bert 3e-5 \\\n",
        "    --weight_decay 1e-4 \\\n",
        "    --freeze_layers 0 \\\n",
        "    --margin 0.2 \\\n",
        "    --batch_size 16 \\\n",
        "    --grad_accum 8 \\\n",
        "    --epochs 150 \\\n",
        "    --final_training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXHzdz9s74Wn",
        "outputId": "ce8177c7-362f-445b-d16c-f85af338ac20"
      },
      "id": "SXHzdz9s74Wn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ğŸš€ DUAL ENCODER TRAINING - FINAL (train+val)\n",
            "======================================================================\n",
            "Device      : cuda\n",
            "â˜ï¸ Google Drive: MONTÃ‰ (/content/drive/MyDrive/Altegrad_Results/dual_encoder_checkpoints)\n",
            "LR GNN      : 0.0008\n",
            "LR BERT     : 3e-05\n",
            "Weight Decay: 0.0001\n",
            "Freeze Layers: 0\n",
            "Margin      : 0.2\n",
            "Batch Size  : 16 Ã— 8 = 128 (effective)\n",
            "Epochs      : 150\n",
            "======================================================================\n",
            "tokenizer_config.json: 100% 339/339 [00:00<00:00, 2.72MB/s]\n",
            "config.json: 100% 612/612 [00:00<00:00, 4.96MB/s]\n",
            "vocab.txt: 228kB [00:00, 40.8MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 1.25MB/s]\n",
            "ğŸ“Š Training sur 32008 exemples (train: 31008, val: 1000)\n",
            "2026-01-12 11:34:24.736229: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-12 11:34:24.758838: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768217664.784672    3773 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768217664.792016    3773 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768217664.810763    3773 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768217664.810790    3773 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768217664.810794    3773 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768217664.810796    3773 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-12 11:34:24.815978: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 440M/440M [00:02<00:00, 166MB/s]\n",
            "Some weights of BertModel were not initialized from the model checkpoint at recobo/chemical-bert-uncased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/content/BornToOverfit/data_baseline/train_dual_encoder_optimized.py:318: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "\n",
            "ğŸ‹ï¸ DÃ©but de l'entraÃ®nement...\n",
            "\n",
            "/content/BornToOverfit/data_baseline/train_dual_encoder_optimized.py:334: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Epoch 1/150 | Train Loss: 0.2083\n",
            "Epoch 2/150 | Train Loss: 0.1713\n",
            "Epoch 3/150 | Train Loss: 0.1113\n",
            "Epoch 4/150 | Train Loss: 0.0755\n",
            "Epoch 5/150 | Train Loss: 0.0560\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep5.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep5.pt (433.8 MB)\n",
            "Epoch 6/150 | Train Loss: 0.0452\n",
            "Epoch 7/150 | Train Loss: 0.0359\n",
            "Epoch 8/150 | Train Loss: 0.0307\n",
            "Epoch 9/150 | Train Loss: 0.0265\n",
            "Epoch 10/150 | Train Loss: 0.0233\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep10.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep10.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final.json (0.0 MB)\n",
            "Epoch 11/150 | Train Loss: 0.0210\n",
            "Epoch 12/150 | Train Loss: 0.0197\n",
            "Epoch 13/150 | Train Loss: 0.0183\n",
            "Epoch 14/150 | Train Loss: 0.0158\n",
            "Epoch 15/150 | Train Loss: 0.0140\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep15.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep15.pt (433.8 MB)\n",
            "Epoch 16/150 | Train Loss: 0.0130\n",
            "Epoch 17/150 | Train Loss: 0.0122\n",
            "Epoch 18/150 | Train Loss: 0.0108\n",
            "Epoch 19/150 | Train Loss: 0.0107\n",
            "Epoch 20/150 | Train Loss: 0.0101\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep20.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep20.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final.json (0.0 MB)\n",
            "Epoch 21/150 | Train Loss: 0.0091\n",
            "Epoch 22/150 | Train Loss: 0.0093\n",
            "Epoch 23/150 | Train Loss: 0.0084\n",
            "Epoch 24/150 | Train Loss: 0.0086\n",
            "Epoch 25/150 | Train Loss: 0.0081\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep25.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep25.pt (433.8 MB)\n",
            "Epoch 26/150 | Train Loss: 0.0082\n",
            "Epoch 27/150 | Train Loss: 0.0077\n",
            "Epoch 28/150 | Train Loss: 0.0076\n",
            "Epoch 29/150 | Train Loss: 0.0076\n",
            "Epoch 30/150 | Train Loss: 0.0075\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep30.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep30.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final.json (0.0 MB)\n",
            "Epoch 31/150 | Train Loss: 0.0072\n",
            "Epoch 32/150 | Train Loss: 0.0068\n",
            "Epoch 33/150 | Train Loss: 0.0068\n",
            "Epoch 34/150 | Train Loss: 0.0063\n",
            "Epoch 35/150 | Train Loss: 0.0067\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep35.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep35.pt (433.8 MB)\n",
            "Epoch 36/150 | Train Loss: 0.0066\n",
            "Epoch 37/150 | Train Loss: 0.0063\n",
            "Epoch 38/150 | Train Loss: 0.0056\n",
            "Epoch 39/150 | Train Loss: 0.0060\n",
            "Epoch 40/150 | Train Loss: 0.0057\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep40.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep40.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final.json (0.0 MB)\n",
            "Epoch 41/150 | Train Loss: 0.0055\n",
            "Epoch 42/150 | Train Loss: 0.0056\n",
            "Epoch 43/150 | Train Loss: 0.0052\n",
            "Epoch 44/150 | Train Loss: 0.0048\n",
            "Epoch 45/150 | Train Loss: 0.0049\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep45.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep45.pt (433.8 MB)\n",
            "Epoch 46/150 | Train Loss: 0.0051\n",
            "Epoch 47/150 | Train Loss: 0.0046\n",
            "Epoch 48/150 | Train Loss: 0.0051\n",
            "Epoch 49/150 | Train Loss: 0.0051\n",
            "Epoch 50/150 | Train Loss: 0.0047\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep50.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep50.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final.json (0.0 MB)\n",
            "Epoch 51/150 | Train Loss: 0.0047\n",
            "Epoch 52/150 | Train Loss: 0.0042\n",
            "Epoch 53/150 | Train Loss: 0.0042\n",
            "Epoch 54/150 | Train Loss: 0.0045\n",
            "Epoch 55/150 | Train Loss: 0.0042\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep55.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep55.pt (433.8 MB)\n",
            "Epoch 56/150 | Train Loss: 0.0040\n",
            "Epoch 57/150 | Train Loss: 0.0041\n",
            "Epoch 58/150 | Train Loss: 0.0043\n",
            "Epoch 59/150 | Train Loss: 0.0040\n",
            "Epoch 60/150 | Train Loss: 0.0036\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep60.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep60.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final.json (0.0 MB)\n",
            "Epoch 61/150 | Train Loss: 0.0039\n",
            "Epoch 62/150 | Train Loss: 0.0036\n",
            "Epoch 63/150 | Train Loss: 0.0036\n",
            "Epoch 64/150 | Train Loss: 0.0035\n",
            "Epoch 65/150 | Train Loss: 0.0039\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep65.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep65.pt (433.8 MB)\n",
            "Epoch 66/150 | Train Loss: 0.0034\n",
            "Epoch 67/150 | Train Loss: 0.0035\n",
            "Epoch 68/150 | Train Loss: 0.0032\n",
            "Epoch 69/150 | Train Loss: 0.0031\n",
            "Epoch 70/150 | Train Loss: 0.0032\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep70.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep70.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final.json (0.0 MB)\n",
            "Epoch 71/150 | Train Loss: 0.0031\n",
            "Epoch 72/150 | Train Loss: 0.0031\n",
            "Epoch 73/150 | Train Loss: 0.0027\n",
            "Epoch 74/150 | Train Loss: 0.0027\n",
            "Epoch 75/150 | Train Loss: 0.0030\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep75.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep75.pt (433.8 MB)\n",
            "Epoch 76/150 | Train Loss: 0.0028\n",
            "Epoch 77/150 | Train Loss: 0.0027\n",
            "Epoch 78/150 | Train Loss: 0.0028\n",
            "Epoch 79/150 | Train Loss: 0.0030\n",
            "Epoch 80/150 | Train Loss: 0.0028\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep80.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep80.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final.json (0.0 MB)\n",
            "Epoch 81/150 | Train Loss: 0.0028\n",
            "Epoch 82/150 | Train Loss: 0.0030\n",
            "Epoch 83/150 | Train Loss: 0.0029\n",
            "Epoch 84/150 | Train Loss: 0.0027\n",
            "Epoch 85/150 | Train Loss: 0.0025\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep85.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep85.pt (433.8 MB)\n",
            "Epoch 86/150 | Train Loss: 0.0023\n",
            "Epoch 87/150 | Train Loss: 0.0026\n",
            "Epoch 88/150 | Train Loss: 0.0022\n",
            "Epoch 89/150 | Train Loss: 0.0023\n",
            "Epoch 90/150 | Train Loss: 0.0025\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep90.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep90.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final.json (0.0 MB)\n",
            "Epoch 91/150 | Train Loss: 0.0023\n",
            "Epoch 92/150 | Train Loss: 0.0022\n",
            "Epoch 93/150 | Train Loss: 0.0023\n",
            "Epoch 94/150 | Train Loss: 0.0022\n",
            "Epoch 95/150 | Train Loss: 0.0020\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep95.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep95.pt (433.8 MB)\n",
            "Epoch 96/150 | Train Loss: 0.0021\n",
            "Epoch 97/150 | Train Loss: 0.0020\n",
            "Epoch 98/150 | Train Loss: 0.0020\n",
            "Epoch 99/150 | Train Loss: 0.0020\n",
            "Epoch 100/150 | Train Loss: 0.0020\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep100.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep100.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final.json (0.0 MB)\n",
            "Epoch 101/150 | Train Loss: 0.0020\n",
            "Epoch 102/150 | Train Loss: 0.0019\n",
            "Epoch 103/150 | Train Loss: 0.0019\n",
            "Epoch 104/150 | Train Loss: 0.0020\n",
            "Epoch 105/150 | Train Loss: 0.0018\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep105.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep105.pt (433.8 MB)\n",
            "Epoch 106/150 | Train Loss: 0.0018\n",
            "Epoch 107/150 | Train Loss: 0.0017\n",
            "Epoch 108/150 | Train Loss: 0.0019\n",
            "Epoch 109/150 | Train Loss: 0.0019\n",
            "Epoch 110/150 | Train Loss: 0.0017\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep110.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep110.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final.json (0.0 MB)\n",
            "Epoch 111/150 | Train Loss: 0.0019\n",
            "Epoch 112/150 | Train Loss: 0.0017\n",
            "Epoch 113/150 | Train Loss: 0.0016\n",
            "Epoch 114/150 | Train Loss: 0.0019\n",
            "Epoch 115/150 | Train Loss: 0.0017\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep115.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep115.pt (433.8 MB)\n",
            "Epoch 116/150 | Train Loss: 0.0016\n",
            "Epoch 117/150 | Train Loss: 0.0016\n",
            "Epoch 118/150 | Train Loss: 0.0020\n",
            "Epoch 119/150 | Train Loss: 0.0015\n",
            "Epoch 120/150 | Train Loss: 0.0016\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep120.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep120.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final.json (0.0 MB)\n",
            "Epoch 121/150 | Train Loss: 0.0015\n",
            "Epoch 122/150 | Train Loss: 0.0017\n",
            "Epoch 123/150 | Train Loss: 0.0013\n",
            "Epoch 124/150 | Train Loss: 0.0014\n",
            "Epoch 125/150 | Train Loss: 0.0016\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep125.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep125.pt (433.8 MB)\n",
            "Epoch 126/150 | Train Loss: 0.0014\n",
            "Epoch 127/150 | Train Loss: 0.0013\n",
            "Epoch 128/150 | Train Loss: 0.0014\n",
            "Epoch 129/150 | Train Loss: 0.0016\n",
            "Epoch 130/150 | Train Loss: 0.0014\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep130.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep130.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final.json (0.0 MB)\n",
            "Epoch 131/150 | Train Loss: 0.0014\n",
            "Epoch 132/150 | Train Loss: 0.0015\n",
            "Epoch 133/150 | Train Loss: 0.0016\n",
            "Epoch 134/150 | Train Loss: 0.0015\n",
            "Epoch 135/150 | Train Loss: 0.0013\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep135.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep135.pt (433.8 MB)\n",
            "Epoch 136/150 | Train Loss: 0.0015\n",
            "Epoch 137/150 | Train Loss: 0.0015\n",
            "Epoch 138/150 | Train Loss: 0.0016\n",
            "Epoch 139/150 | Train Loss: 0.0012\n",
            "Epoch 140/150 | Train Loss: 0.0013\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep140.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep140.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final.json (0.0 MB)\n",
            "Epoch 141/150 | Train Loss: 0.0012\n",
            "Epoch 142/150 | Train Loss: 0.0013\n",
            "Epoch 143/150 | Train Loss: 0.0013\n",
            "Epoch 144/150 | Train Loss: 0.0013\n",
            "Epoch 145/150 | Train Loss: 0.0013\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep145.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep145.pt (433.8 MB)\n",
            "Epoch 146/150 | Train Loss: 0.0014\n",
            "Epoch 147/150 | Train Loss: 0.0014\n",
            "Epoch 148/150 | Train Loss: 0.0015\n",
            "Epoch 149/150 | Train Loss: 0.0013\n",
            "Epoch 150/150 | Train Loss: 0.0014\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep150.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep150.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final.json (0.0 MB)\n",
            "\n",
            "======================================================================\n",
            "âœ… EntraÃ®nement terminÃ© !\n",
            "ğŸ“ Logs sauvegardÃ©s dans : data_baseline/data/training_log_final.json\n",
            "â˜ï¸ Backups disponibles sur : /content/drive/MyDrive/Altegrad_Results/dual_encoder_checkpoints\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "___"
      ],
      "metadata": {
        "id": "s8kmM1Gju5H3"
      },
      "id": "s8kmM1Gju5H3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "___"
      ],
      "metadata": {
        "id": "WJfOnJ47-AwH"
      },
      "id": "WJfOnJ47-AwH"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip -q install -U evaluate bert-score sacrebleu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfvTZSwjMYJF",
        "outputId": "c7590989-3e5c-4f79-d522-00e166e4a510"
      },
      "id": "NfvTZSwjMYJF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/61.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/100.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code pour avoir les rÃ©sultats et convertir au format csv pour kaggle"
      ],
      "metadata": {
        "id": "xcXIGfyPXStJ"
      },
      "id": "xcXIGfyPXStJ"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch_geometric.data import Batch\n",
        "from torch_geometric.nn import GPSConv, GINEConv, global_add_pool\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import evaluate\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==============================================================================\n",
        "# 0. CONFIGURATION\n",
        "# ==============================================================================\n",
        "#\n",
        "#MODEL_PATH = \"data_baseline/data/dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep250.pt\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Altegrad_Results/dual_encoder_checkpoints/dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep150.pt\"\n",
        "DATA_DIR = Path(\"data_baseline/data\")\n",
        "MODEL_NAME = \"recobo/chemical-bert-uncased\"\n",
        "BATCH_SIZE = 32\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Output\n",
        "SUBMISSION_PATH = \"submission_dual_encoder_final.csv\"\n",
        "\n",
        "print(f\"âš™ï¸ Configuration : {DEVICE}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. DÃ‰FINITION DE L'ARCHITECTURE\n",
        "# ==============================================================================\n",
        "ATOM_DIMS = [119, 4, 11, 12, 9, 5, 8, 2, 2]\n",
        "BOND_DIMS = [22, 6, 2]\n",
        "\n",
        "class AtomEncoder(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.ModuleList([nn.Embedding(dim, hidden_dim) for dim in ATOM_DIMS])\n",
        "    def forward(self, x):\n",
        "        return sum(emb(x[:, i]) for i, emb in enumerate(self.embeddings))\n",
        "\n",
        "class BondEncoder(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.ModuleList([nn.Embedding(dim, hidden_dim) for dim in BOND_DIMS])\n",
        "    def forward(self, edge_attr):\n",
        "        return sum(emb(edge_attr[:, i]) for i, emb in enumerate(self.embeddings))\n",
        "\n",
        "class MolGNN(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, out_dim=768, num_layers=4, num_heads=8, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.atom_encoder = AtomEncoder(hidden_dim)\n",
        "        self.bond_encoder = BondEncoder(hidden_dim)\n",
        "        self.convs = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            local_nn = nn.Sequential(\n",
        "                nn.Linear(hidden_dim, 2 * hidden_dim),\n",
        "                nn.BatchNorm1d(2 * hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(2 * hidden_dim, hidden_dim),\n",
        "            )\n",
        "            self.convs.append(GPSConv(\n",
        "                hidden_dim,\n",
        "                GINEConv(local_nn, train_eps=True, edge_dim=hidden_dim),\n",
        "                heads=num_heads,\n",
        "                dropout=dropout,\n",
        "                attn_type='multihead'\n",
        "            ))\n",
        "        self.pool = global_add_pool\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, batch):\n",
        "        h = self.atom_encoder(batch.x)\n",
        "        edge_attr = self.bond_encoder(batch.edge_attr)\n",
        "        for conv in self.convs:\n",
        "            h = conv(h, batch.edge_index, batch.batch, edge_attr=edge_attr)\n",
        "        return self.proj(self.pool(h, batch.batch))\n",
        "\n",
        "class DualEncoder(nn.Module):\n",
        "    def __init__(self, model_name, gnn_args, freeze_layers=0):\n",
        "        super().__init__()\n",
        "        self.graph_encoder = MolGNN(**gnn_args)\n",
        "        self.text_encoder = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        if freeze_layers > 0:\n",
        "            print(f\"â„ï¸ Gel des {freeze_layers} premiÃ¨res couches de BERT\")\n",
        "            for param in self.text_encoder.embeddings.parameters():\n",
        "                param.requires_grad = False\n",
        "            for i in range(freeze_layers):\n",
        "                if i < len(self.text_encoder.encoder.layer):\n",
        "                    for param in self.text_encoder.encoder.layer[i].parameters():\n",
        "                        param.requires_grad = False\n",
        "\n",
        "        bert_dim = self.text_encoder.config.hidden_size\n",
        "        out_dim = gnn_args['out_dim']\n",
        "        self.text_proj = nn.Linear(bert_dim, out_dim) if bert_dim != out_dim else nn.Identity()\n",
        "\n",
        "    def forward_text(self, input_ids, attention_mask):\n",
        "        t_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return F.normalize(self.text_proj(t_out.last_hidden_state[:, 0, :]), dim=-1)\n",
        "\n",
        "    def forward_graph(self, batch):\n",
        "        return F.normalize(self.graph_encoder(batch), dim=-1)\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FONCTIONS DE CHARGEMENT\n",
        "# ==============================================================================\n",
        "GNN_CONF_FORCE = {\n",
        "    'hidden_dim': 256,\n",
        "    'out_dim': 768,\n",
        "    'num_layers': 4,\n",
        "    'num_heads': 8,\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "# 2. MODIFICATION DE LA FONCTION DE CHARGEMENT\n",
        "def load_dual_encoder(checkpoint_path, device='cuda'):\n",
        "    \"\"\"Charge le DualEncoder avec la config forcÃ©e de ton entraÃ®nement.\"\"\"\n",
        "\n",
        "    print(f\"ğŸ“‚ Lecture du checkpoint : {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # RÃ©cupÃ©ration du state_dict (poids)\n",
        "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "        state_dict = checkpoint['model_state_dict']\n",
        "        # On ignore les args sauvegardÃ©s pour utiliser ceux qu'on est sÃ»rs d'avoir utilisÃ©s\n",
        "        print(f\"   â„¹ï¸ Info Checkpoint - Epoch: {checkpoint.get('epoch', '?')}, Loss: {checkpoint.get('train_loss', '?')}\")\n",
        "    else:\n",
        "        state_dict = checkpoint\n",
        "        print(f\"   âš ï¸ Format ancien ou direct state_dict dÃ©tectÃ©\")\n",
        "\n",
        "    # Initialisation du modÃ¨le avec LA BONNE CONFIGURATION\n",
        "    # On utilise GNN_CONF_FORCE dÃ©fini plus haut\n",
        "    model = DualEncoder(MODEL_NAME, GNN_CONF_FORCE, freeze_layers=0)\n",
        "\n",
        "    # Chargement des poids\n",
        "    # strict=False peut Ãªtre utile si tu as des soucis de prÃ©fixes \"module.\",\n",
        "\n",
        "    try:\n",
        "        model.load_state_dict(state_dict)\n",
        "        print(\"âœ… Poids chargÃ©s avec succÃ¨s (strict=True)\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"âš ï¸ Erreur de chargement stricte, tentative avec strict=False... ({e})\")\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "        print(\"âœ… Poids chargÃ©s (strict=False)\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    return model, GNN_CONF_FORCE\n",
        "\n",
        "\n",
        "def load_texts_and_graphs(pkl_path, has_description=True):\n",
        "    \"\"\"\n",
        "    Charge les graphes et extrait les descriptions (si disponibles).\n",
        "\n",
        "    Args:\n",
        "        pkl_path: Chemin vers le fichier pickle\n",
        "        has_description: Si False, retourne None pour les textes\n",
        "    \"\"\"\n",
        "    with open(pkl_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    if has_description:\n",
        "        texts = [d.description for d in data]\n",
        "    else:\n",
        "        texts = None  # Pas de description dans le test set\n",
        "\n",
        "    return data, texts\n",
        "\n",
        "\n",
        "class GraphDataset(Dataset):\n",
        "    def __init__(self, graphs):\n",
        "        self.graphs = graphs\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.graphs[idx]\n",
        "\n",
        "\n",
        "def collate_text(batch):\n",
        "    return batch\n",
        "\n",
        "\n",
        "def collate_graph(batch):\n",
        "    return Batch.from_data_list(batch)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. MOTEUR D'INFERENCE\n",
        "# ==============================================================================\n",
        "@torch.no_grad()\n",
        "def generate_reference_embeddings(model, tokenizer, texts, device, batch_size=32):\n",
        "    \"\"\"Recalcule les embeddings avec le modÃ¨le fine-tunÃ©.\"\"\"\n",
        "    print(f\"ğŸ”„ Recalcul des embeddings de rÃ©fÃ©rence ({len(texts)} textes)...\")\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "\n",
        "    loader = DataLoader(texts, batch_size=batch_size, collate_fn=collate_text)\n",
        "\n",
        "    for batch_texts in tqdm(loader, desc=\"Encoding Reference Texts\"):\n",
        "        inputs = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors='pt',\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128\n",
        "        ).to(device)\n",
        "\n",
        "        emb = model.forward_text(inputs['input_ids'], inputs['attention_mask'])\n",
        "        embeddings.append(emb.cpu())\n",
        "\n",
        "    return torch.cat(embeddings, dim=0)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_graphs(model, graphs, device, batch_size=32):\n",
        "    \"\"\"Encode les graphes en embeddings.\"\"\"\n",
        "    print(f\"ğŸ§ª Encodage de {len(graphs)} graphes...\")\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "\n",
        "    loader = DataLoader(\n",
        "        GraphDataset(graphs),\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=collate_graph\n",
        "    )\n",
        "\n",
        "    for batch in tqdm(loader, desc=\"Encoding Graphs\"):\n",
        "        batch = batch.to(device)\n",
        "        emb = model.forward_graph(batch)\n",
        "        embeddings.append(emb.cpu())\n",
        "\n",
        "    return torch.cat(embeddings, dim=0)\n",
        "\n",
        "\n",
        "def generate_submission(predictions, output_path):\n",
        "    \"\"\"\n",
        "    GÃ©nÃ¨re le fichier de submission Kaggle.\n",
        "\n",
        "    Args:\n",
        "        predictions: Liste de textes prÃ©dits (dans l'ordre du test set)\n",
        "        output_path: Chemin du fichier CSV Ã  crÃ©er\n",
        "    \"\"\"\n",
        "    submission_df = pd.DataFrame({\n",
        "        'ID': range(len(predictions)),\n",
        "        'description': predictions\n",
        "    })\n",
        "\n",
        "    submission_df.to_csv(output_path, index=False)\n",
        "    print(f\"\\nğŸ’¾ Submission sauvegardÃ©e : {output_path}\")\n",
        "    print(f\"   Nombre de prÃ©dictions : {len(predictions)}\")\n",
        "    print(f\"   Preview :\")\n",
        "    print(submission_df.head(10).to_string(index=False))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_evaluation_and_submission():\n",
        "    \"\"\"Ã‰valuation + GÃ©nÃ©ration de submission Kaggle.\"\"\"\n",
        "\n",
        "    # === 1. CHARGEMENT MODÃˆLE ===\n",
        "    print(f\"ğŸ“¥ Chargement du Dual Encoder depuis {MODEL_PATH}\")\n",
        "    model, config = load_dual_encoder(MODEL_PATH, DEVICE)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    # === 2. CHARGEMENT DONNÃ‰ES ===\n",
        "    print(\"ğŸ“‚ Chargement des datasets...\")\n",
        "\n",
        "    # ğŸ†• CHANGEMENT MAJEUR : Combiner train + val pour la rÃ©fÃ©rence\n",
        "    train_graphs, train_texts = load_texts_and_graphs(DATA_DIR / \"train_graphs.pkl\")\n",
        "    val_graphs, val_texts = load_texts_and_graphs(DATA_DIR / \"validation_graphs.pkl\")\n",
        "\n",
        "    # Combiner train + val\n",
        "    reference_graphs = train_graphs + val_graphs\n",
        "    reference_texts = train_texts + val_texts\n",
        "\n",
        "    print(f\"   Train: {len(train_graphs)} exemples\")\n",
        "    print(f\"   Val:   {len(val_graphs)} exemples\")\n",
        "    print(f\"   ğŸ“š RÃ‰FÃ‰RENCE (train+val): {len(reference_texts)} exemples\")\n",
        "\n",
        "    # Test set pour Kaggle\n",
        "    test_graphs, _ = load_texts_and_graphs(DATA_DIR / \"test_graphs.pkl\", has_description=False)\n",
        "    print(f\"   Test:  {len(test_graphs)} exemples (pour Kaggle)\")\n",
        "\n",
        "    # === 3. CRÃ‰ATION BANQUE DE RÃ‰FÃ‰RENCE (TRAIN+VAL) ===\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ğŸ”„ ENCODAGE DE LA BANQUE DE RÃ‰FÃ‰RENCE (TRAIN+VAL)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    reference_embs = generate_reference_embeddings(\n",
        "        model, tokenizer, reference_texts, DEVICE, batch_size=BATCH_SIZE\n",
        "    )\n",
        "    reference_embs = reference_embs.to(DEVICE)\n",
        "\n",
        "    print(f\"   âœ… Embeddings rÃ©fÃ©rence: {reference_embs.shape}\")\n",
        "\n",
        "    # === 4. GÃ‰NÃ‰RATION PRÃ‰DICTIONS TEST (pour Kaggle) ===\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ğŸ¯ GÃ‰NÃ‰RATION SUBMISSION KAGGLE (TEST SET)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    test_mol_embs = encode_graphs(model, test_graphs, DEVICE, batch_size=BATCH_SIZE)\n",
        "    test_mol_embs = test_mol_embs.to(DEVICE)\n",
        "\n",
        "    print(\"ğŸ” Recherche des descriptions les plus proches (Test)...\")\n",
        "    sims_test = test_mol_embs @ reference_embs.t()\n",
        "    best_indices_test = sims_test.argmax(dim=-1).cpu().numpy()\n",
        "\n",
        "    preds_test = [reference_texts[idx] for idx in best_indices_test]\n",
        "\n",
        "    # GÃ©nÃ©ration du fichier submission\n",
        "    generate_submission(preds_test, SUBMISSION_PATH)\n",
        "\n",
        "    # Exemples de prÃ©dictions\n",
        "    print(\"\\nğŸ“ EXEMPLES DE PRÃ‰DICTIONS (TEST SET) :\")\n",
        "    print(\"-\" * 70)\n",
        "    for i in range(min(5, len(preds_test))):\n",
        "        print(f\"\\nId {i}:\")\n",
        "        print(f\"  {preds_test[i][:120]}...\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    return {\n",
        "        'submission_file': SUBMISSION_PATH,\n",
        "        'num_references': len(reference_texts),\n",
        "        'num_predictions': len(preds_test)\n",
        "    }\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_evaluation_and_submission()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"âœ… TERMINÃ‰ !\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"ğŸ“„ Fichier de submission : {results['submission_file']}\")\n",
        "    print(f\"ğŸ“š Banque de rÃ©fÃ©rence   : {results['num_references']} descriptions\")\n",
        "    print(f\"ğŸ¯ PrÃ©dictions gÃ©nÃ©rÃ©es  : {results['num_predictions']}\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nğŸ’¡ go Upload le fichier sur Kaggle !\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAoKOo_99rF9",
        "outputId": "bf18ad47-1a3d-4943-a1db-4eecbbcea3af"
      },
      "id": "pAoKOo_99rF9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš™ï¸ Configuration : cuda\n",
            "ğŸ“¥ Chargement du Dual Encoder depuis /content/drive/MyDrive/Altegrad_Results/dual_encoder_checkpoints/dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep150.pt\n",
            "ğŸ“‚ Lecture du checkpoint : /content/drive/MyDrive/Altegrad_Results/dual_encoder_checkpoints/dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep150.pt\n",
            "   â„¹ï¸ Info Checkpoint - Epoch: 150, Loss: 0.0013596335987279798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at recobo/chemical-bert-uncased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Poids chargÃ©s avec succÃ¨s (strict=True)\n",
            "ğŸ“‚ Chargement des datasets...\n",
            "   Train: 31008 exemples\n",
            "   Val:   1000 exemples\n",
            "   ğŸ“š RÃ‰FÃ‰RENCE (train+val): 32008 exemples\n",
            "   Test:  1000 exemples (pour Kaggle)\n",
            "\n",
            "======================================================================\n",
            "ğŸ”„ ENCODAGE DE LA BANQUE DE RÃ‰FÃ‰RENCE (TRAIN+VAL)\n",
            "======================================================================\n",
            "ğŸ”„ Recalcul des embeddings de rÃ©fÃ©rence (32008 textes)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding Reference Texts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1001/1001 [01:46<00:00,  9.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ… Embeddings rÃ©fÃ©rence: torch.Size([32008, 768])\n",
            "\n",
            "======================================================================\n",
            "ğŸ¯ GÃ‰NÃ‰RATION SUBMISSION KAGGLE (TEST SET)\n",
            "======================================================================\n",
            "ğŸ§ª Encodage de 1000 graphes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding Graphs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 70.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Recherche des descriptions les plus proches (Test)...\n",
            "\n",
            "ğŸ’¾ Submission sauvegardÃ©e : submission_dual_encoder_final.csv\n",
            "   Nombre de prÃ©dictions : 1000\n",
            "   Preview :\n",
            " ID                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         description\n",
            "  0                                                                                                                                                                                                                                                                                                  The molecule is a glycodihydroceramide that is a sphinganine derivative having a D-glucosyl group at the 1-position and a tetracosanoyl group attached to the nitrogen. It has a role as a hapten.\n",
            "  1 The molecule is the monohydrate form of doxapram hydrochloride. A central and respiratory stimulant with a brief duration of action, it is used as a temporary treatment of acute respiratory failure, particularly when superimposed on chronic obstructive pulmonary disease, and of postoperative respiratory depression. It has also been used for treatment of postoperative shivering. It has a role as a central nervous system stimulant. It contains a doxapram hydrochloride (anhydrous).\n",
            "  2                                                                              The molecule is a steroid glucosiduronic acid that is 5alpha-androstane-3beta,17beta-diol having a single beta-D-glucuronic acid residue attached at position 17. It is a steroid glucosiduronic acid, a beta-D-glucosiduronic acid and a 3beta-hydroxy steroid. It derives from a 5alpha-androstane-3beta,17beta-diol. It is a conjugate acid of a 5alpha-androstane-3beta,17beta-diol 17-O-(beta-D-glucuronide)(1-).\n",
            "  3                                                                                                                                                                                                                                                                                         The molecule is a hydroxy fatty acid ascaroside anion that is the conjugate base of oscr#37, obtained by deprotonation of the carboxy group; major species at pH 7.3. It is a conjugate base of an oscr#37.\n",
            "  4                                                                                                                                                                                                                                      The molecule is an organochlorine compound that consists of acetaldehyde where all the methyl hydrogens are replaced by chloro groups. It has a role as a mouse metabolite. It is an organochlorine compound and an aldehyde. It derives from an acetaldehyde.\n",
            "  5                                                                                                                                                                                      The molecule is an amino trisaccharide comprised of an N-acetylated glucosamine residue, sulfated on O-6, between two galactosyl residues. It is an intermediate in the keratan sulfate degradation pathway. It has a role as a mouse metabolite. It is an amino trisaccharide and an oligosaccharide sulfate.\n",
            "  6                                                                                                                                                                                                                                                                                          The molecule is a galactonic acid compound having L-configuration. It has a role as an Escherichia coli metabolite. It is a conjugate acid of a L-galactonate. It is an enantiomer of a D-galactonic acid.\n",
            "  7                                                                                                    The molecule is an eighteen-membered homodetic cyclic peptide which is isolated from Oscillatoria sp. and exhibits antimalarial activity against the W2 chloroquine-resistant strain of the malarial parasite, Plasmodium falciparum. It has a role as a metabolite and an antimalarial. It is a homodetic cyclic peptide, a member of 1,3-oxazoles, a member of 1,3-thiazoles and a macrocycle.\n",
            "  8                                                                                                                                                                                                                                                                                        The molecule is a monocarboxylic acid that is propanoic acid substituted by a 3,4-dimethoxyphenyl group at position 3. It is a monocarboxylic acid and a dimethoxybenzene. It derives from a propionic acid.\n",
            "  9                                                                                                                                                                                                                            The molecule is an organic cation obtained by protonation of the two free amino groups of 2'-deamino-2'-hydroxyparomamine; major species at pH 7.3. It is an ammonium ion derivative and an organic cation. It is a conjugate acid of a 2'-deamino-2'-hydroxyparomamine.\n",
            "\n",
            "ğŸ“ EXEMPLES DE PRÃ‰DICTIONS (TEST SET) :\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Id 0:\n",
            "  The molecule is a glycodihydroceramide that is a sphinganine derivative having a D-glucosyl group at the 1-position and ...\n",
            "\n",
            "Id 1:\n",
            "  The molecule is the monohydrate form of doxapram hydrochloride. A central and respiratory stimulant with a brief duratio...\n",
            "\n",
            "Id 2:\n",
            "  The molecule is a steroid glucosiduronic acid that is 5alpha-androstane-3beta,17beta-diol having a single beta-D-glucuro...\n",
            "\n",
            "Id 3:\n",
            "  The molecule is a hydroxy fatty acid ascaroside anion that is the conjugate base of oscr#37, obtained by deprotonation o...\n",
            "\n",
            "Id 4:\n",
            "  The molecule is an organochlorine compound that consists of acetaldehyde where all the methyl hydrogens are replaced by ...\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "======================================================================\n",
            "âœ… TERMINÃ‰ !\n",
            "======================================================================\n",
            "ğŸ“„ Fichier de submission : submission_dual_encoder_final.csv\n",
            "ğŸ“š Banque de rÃ©fÃ©rence   : 32008 descriptions\n",
            "ğŸ¯ PrÃ©dictions gÃ©nÃ©rÃ©es  : 1000\n",
            "======================================================================\n",
            "\n",
            "ğŸ’¡ Prochaine Ã©tape : Upload le fichier sur Kaggle !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch_geometric.data import Batch\n",
        "from torch_geometric.nn import GPSConv, GINEConv, global_add_pool\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import evaluate\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==============================================================================\n",
        "# 0. CONFIGURATION\n",
        "# ==============================================================================\n",
        "#MODEL_PATH = \"data_baseline/data/dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep250.pt\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Altegrad_Results/dual_encoder_checkpoints_head4/dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep150.pt\"\n",
        "DATA_DIR = Path(\"data_baseline/data\")\n",
        "MODEL_NAME = \"recobo/chemical-bert-uncased\"\n",
        "BATCH_SIZE = 32\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Output\n",
        "SUBMISSION_PATH = \"submission_dual_encoder_final.csv\"\n",
        "\n",
        "print(f\"âš™ï¸ Configuration : {DEVICE}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. DÃ‰FINITION DE L'ARCHITECTURE\n",
        "# ==============================================================================\n",
        "ATOM_DIMS = [119, 4, 11, 12, 9, 5, 8, 2, 2]\n",
        "BOND_DIMS = [22, 6, 2]\n",
        "\n",
        "class AtomEncoder(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.ModuleList([nn.Embedding(dim, hidden_dim) for dim in ATOM_DIMS])\n",
        "    def forward(self, x):\n",
        "        return sum(emb(x[:, i]) for i, emb in enumerate(self.embeddings))\n",
        "\n",
        "class BondEncoder(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.ModuleList([nn.Embedding(dim, hidden_dim) for dim in BOND_DIMS])\n",
        "    def forward(self, edge_attr):\n",
        "        return sum(emb(edge_attr[:, i]) for i, emb in enumerate(self.embeddings))\n",
        "\n",
        "class MolGNN(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, out_dim=768, num_layers=4, num_heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.atom_encoder = AtomEncoder(hidden_dim)\n",
        "        self.bond_encoder = BondEncoder(hidden_dim)\n",
        "        self.convs = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            local_nn = nn.Sequential(\n",
        "                nn.Linear(hidden_dim, 2 * hidden_dim),\n",
        "                nn.BatchNorm1d(2 * hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(2 * hidden_dim, hidden_dim),\n",
        "            )\n",
        "            self.convs.append(GPSConv(\n",
        "                hidden_dim,\n",
        "                GINEConv(local_nn, train_eps=True, edge_dim=hidden_dim),\n",
        "                heads=num_heads,\n",
        "                dropout=dropout,\n",
        "                attn_type='multihead'\n",
        "            ))\n",
        "        self.pool = global_add_pool\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, batch):\n",
        "        h = self.atom_encoder(batch.x)\n",
        "        edge_attr = self.bond_encoder(batch.edge_attr)\n",
        "        for conv in self.convs:\n",
        "            h = conv(h, batch.edge_index, batch.batch, edge_attr=edge_attr)\n",
        "        return self.proj(self.pool(h, batch.batch))\n",
        "\n",
        "class DualEncoder(nn.Module):\n",
        "    def __init__(self, model_name, gnn_args, freeze_layers=0):\n",
        "        super().__init__()\n",
        "        self.graph_encoder = MolGNN(**gnn_args)\n",
        "        self.text_encoder = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        if freeze_layers > 0:\n",
        "            print(f\"â„ï¸ Gel des {freeze_layers} premiÃ¨res couches de BERT\")\n",
        "            for param in self.text_encoder.embeddings.parameters():\n",
        "                param.requires_grad = False\n",
        "            for i in range(freeze_layers):\n",
        "                if i < len(self.text_encoder.encoder.layer):\n",
        "                    for param in self.text_encoder.encoder.layer[i].parameters():\n",
        "                        param.requires_grad = False\n",
        "\n",
        "        bert_dim = self.text_encoder.config.hidden_size\n",
        "        out_dim = gnn_args['out_dim']\n",
        "        self.text_proj = nn.Linear(bert_dim, out_dim) if bert_dim != out_dim else nn.Identity()\n",
        "\n",
        "    def forward_text(self, input_ids, attention_mask):\n",
        "        t_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return F.normalize(self.text_proj(t_out.last_hidden_state[:, 0, :]), dim=-1)\n",
        "\n",
        "    def forward_graph(self, batch):\n",
        "        return F.normalize(self.graph_encoder(batch), dim=-1)\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FONCTIONS DE CHARGEMENT\n",
        "# ==============================================================================\n",
        "GNN_CONF_FORCE = {\n",
        "    'hidden_dim': 256,\n",
        "    'out_dim': 768,\n",
        "    'num_layers': 4,\n",
        "    'num_heads': 4,\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "# 2.\n",
        "def load_dual_encoder(checkpoint_path, device='cuda'):\n",
        "    \"\"\"Charge le DualEncoder avec la config forcÃ©e de ton entraÃ®nement.\"\"\"\n",
        "\n",
        "    print(f\"ğŸ“‚ Lecture du checkpoint : {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # RÃ©cupÃ©ration du state_dict (poids)\n",
        "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "        state_dict = checkpoint['model_state_dict']\n",
        "        print(f\"   â„¹ï¸ Info Checkpoint - Epoch: {checkpoint.get('epoch', '?')}, Loss: {checkpoint.get('train_loss', '?')}\")\n",
        "    else:\n",
        "        state_dict = checkpoint\n",
        "        print(f\"   âš ï¸ Format ancien ou direct state_dict dÃ©tectÃ©\")\n",
        "\n",
        "    # Initialisation du modÃ¨le avec LA BONNE CONFIGURATION\n",
        "    # On utilise GNN_CONF_FORCE dÃ©fini plus haut\n",
        "    model = DualEncoder(MODEL_NAME, GNN_CONF_FORCE, freeze_layers=0)\n",
        "\n",
        "    # Chargement des poids\n",
        "    try:\n",
        "        model.load_state_dict(state_dict)\n",
        "        print(\"âœ… Poids chargÃ©s avec succÃ¨s (strict=True)\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"âš ï¸ Erreur de chargement stricte, tentative avec strict=False... ({e})\")\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "        print(\"âœ… Poids chargÃ©s (strict=False)\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    return model, GNN_CONF_FORCE\n",
        "\n",
        "\n",
        "def load_texts_and_graphs(pkl_path, has_description=True):\n",
        "    \"\"\"\n",
        "    Charge les graphes et extrait les descriptions (si disponibles).\n",
        "\n",
        "    Args:\n",
        "        pkl_path: Chemin vers le fichier pickle\n",
        "        has_description: Si False, retourne None pour les textes\n",
        "    \"\"\"\n",
        "    with open(pkl_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    if has_description:\n",
        "        texts = [d.description for d in data]\n",
        "    else:\n",
        "        texts = None  # Pas de description dans le test set\n",
        "\n",
        "    return data, texts\n",
        "\n",
        "\n",
        "class GraphDataset(Dataset):\n",
        "    def __init__(self, graphs):\n",
        "        self.graphs = graphs\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.graphs[idx]\n",
        "\n",
        "\n",
        "def collate_text(batch):\n",
        "    return batch\n",
        "\n",
        "\n",
        "def collate_graph(batch):\n",
        "    return Batch.from_data_list(batch)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. MOTEUR D'INFERENCE\n",
        "# ==============================================================================\n",
        "@torch.no_grad()\n",
        "def generate_reference_embeddings(model, tokenizer, texts, device, batch_size=32):\n",
        "    \"\"\"Recalcule les embeddings avec le modÃ¨le fine-tunÃ©.\"\"\"\n",
        "    print(f\"ğŸ”„ Recalcul des embeddings de rÃ©fÃ©rence ({len(texts)} textes)...\")\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "\n",
        "    loader = DataLoader(texts, batch_size=batch_size, collate_fn=collate_text)\n",
        "\n",
        "    for batch_texts in tqdm(loader, desc=\"Encoding Reference Texts\"):\n",
        "        inputs = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors='pt',\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128\n",
        "        ).to(device)\n",
        "\n",
        "        emb = model.forward_text(inputs['input_ids'], inputs['attention_mask'])\n",
        "        embeddings.append(emb.cpu())\n",
        "\n",
        "    return torch.cat(embeddings, dim=0)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_graphs(model, graphs, device, batch_size=32):\n",
        "    \"\"\"Encode les graphes en embeddings.\"\"\"\n",
        "    print(f\"ğŸ§ª Encodage de {len(graphs)} graphes...\")\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "\n",
        "    loader = DataLoader(\n",
        "        GraphDataset(graphs),\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=collate_graph\n",
        "    )\n",
        "\n",
        "    for batch in tqdm(loader, desc=\"Encoding Graphs\"):\n",
        "        batch = batch.to(device)\n",
        "        emb = model.forward_graph(batch)\n",
        "        embeddings.append(emb.cpu())\n",
        "\n",
        "    return torch.cat(embeddings, dim=0)\n",
        "\n",
        "\n",
        "def generate_submission(predictions, output_path):\n",
        "    \"\"\"\n",
        "    GÃ©nÃ¨re le fichier de submission Kaggle.\n",
        "\n",
        "    Args:\n",
        "        predictions: Liste de textes prÃ©dits (dans l'ordre du test set)\n",
        "        output_path: Chemin du fichier CSV Ã  crÃ©er\n",
        "    \"\"\"\n",
        "    submission_df = pd.DataFrame({\n",
        "        'ID': range(len(predictions)),\n",
        "        'description': predictions\n",
        "    })\n",
        "\n",
        "    submission_df.to_csv(output_path, index=False)\n",
        "    print(f\"\\nğŸ’¾ Submission sauvegardÃ©e : {output_path}\")\n",
        "    print(f\"   Nombre de prÃ©dictions : {len(predictions)}\")\n",
        "    print(f\"   Preview :\")\n",
        "    print(submission_df.head(10).to_string(index=False))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_evaluation_and_submission():\n",
        "    \"\"\"Ã‰valuation + GÃ©nÃ©ration de submission Kaggle.\"\"\"\n",
        "\n",
        "    # === 1. CHARGEMENT MODÃˆLE ===\n",
        "    print(f\"ğŸ“¥ Chargement du Dual Encoder depuis {MODEL_PATH}\")\n",
        "    model, config = load_dual_encoder(MODEL_PATH, DEVICE)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    # === 2. CHARGEMENT DONNÃ‰ES ===\n",
        "    print(\"ğŸ“‚ Chargement des datasets...\")\n",
        "\n",
        "    # ğŸ†• CHANGEMENT MAJEUR : Combiner train + val pour la rÃ©fÃ©rence\n",
        "    train_graphs, train_texts = load_texts_and_graphs(DATA_DIR / \"train_graphs.pkl\")\n",
        "    val_graphs, val_texts = load_texts_and_graphs(DATA_DIR / \"validation_graphs.pkl\")\n",
        "\n",
        "    # Combiner train + val\n",
        "    reference_graphs = train_graphs + val_graphs\n",
        "    reference_texts = train_texts + val_texts\n",
        "\n",
        "    print(f\"   Train: {len(train_graphs)} exemples\")\n",
        "    print(f\"   Val:   {len(val_graphs)} exemples\")\n",
        "    print(f\"   ğŸ“š RÃ‰FÃ‰RENCE (train+val): {len(reference_texts)} exemples\")\n",
        "\n",
        "    # Test set pour Kaggle\n",
        "    test_graphs, _ = load_texts_and_graphs(DATA_DIR / \"test_graphs.pkl\", has_description=False)\n",
        "    print(f\"   Test:  {len(test_graphs)} exemples (pour Kaggle)\")\n",
        "\n",
        "    # === 3. CRÃ‰ATION BANQUE DE RÃ‰FÃ‰RENCE (TRAIN+VAL) ===\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ğŸ”„ ENCODAGE DE LA BANQUE DE RÃ‰FÃ‰RENCE (TRAIN+VAL)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    reference_embs = generate_reference_embeddings(\n",
        "        model, tokenizer, reference_texts, DEVICE, batch_size=BATCH_SIZE\n",
        "    )\n",
        "    reference_embs = reference_embs.to(DEVICE)\n",
        "\n",
        "    print(f\"   âœ… Embeddings rÃ©fÃ©rence: {reference_embs.shape}\")\n",
        "\n",
        "    # === 4. GÃ‰NÃ‰RATION PRÃ‰DICTIONS TEST (pour Kaggle) ===\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ğŸ¯ GÃ‰NÃ‰RATION SUBMISSION KAGGLE (TEST SET)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    test_mol_embs = encode_graphs(model, test_graphs, DEVICE, batch_size=BATCH_SIZE)\n",
        "    test_mol_embs = test_mol_embs.to(DEVICE)\n",
        "\n",
        "    print(\"ğŸ” Recherche des descriptions les plus proches (Test)...\")\n",
        "    sims_test = test_mol_embs @ reference_embs.t()\n",
        "    best_indices_test = sims_test.argmax(dim=-1).cpu().numpy()\n",
        "\n",
        "    preds_test = [reference_texts[idx] for idx in best_indices_test]\n",
        "\n",
        "    # GÃ©nÃ©ration du fichier submission\n",
        "    generate_submission(preds_test, SUBMISSION_PATH)\n",
        "\n",
        "    # Exemples de prÃ©dictions\n",
        "    print(\"\\nğŸ“ EXEMPLES DE PRÃ‰DICTIONS (TEST SET) :\")\n",
        "    print(\"-\" * 70)\n",
        "    for i in range(min(5, len(preds_test))):\n",
        "        print(f\"\\nId {i}:\")\n",
        "        print(f\"  {preds_test[i][:120]}...\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    return {\n",
        "        'submission_file': SUBMISSION_PATH,\n",
        "        'num_references': len(reference_texts),\n",
        "        'num_predictions': len(preds_test)\n",
        "    }\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_evaluation_and_submission()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"âœ… TERMINÃ‰ !\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"ğŸ“„ Fichier de submission : {results['submission_file']}\")\n",
        "    print(f\"ğŸ“š Banque de rÃ©fÃ©rence   : {results['num_references']} descriptions\")\n",
        "    print(f\"ğŸ¯ PrÃ©dictions gÃ©nÃ©rÃ©es  : {results['num_predictions']}\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nğŸ’¡ Prochaine Ã©tape : Upload le fichier sur Kaggle !\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfkF37bnyYMP",
        "outputId": "8666582e-ed49-4b36-dc1f-6af04e8d9517"
      },
      "id": "CfkF37bnyYMP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš™ï¸ Configuration : cuda\n",
            "ğŸ“¥ Chargement du Dual Encoder depuis /content/drive/MyDrive/Altegrad_Results/dual_encoder_checkpoints_head4/dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep150.pt\n",
            "ğŸ“‚ Lecture du checkpoint : /content/drive/MyDrive/Altegrad_Results/dual_encoder_checkpoints_head4/dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep150.pt\n",
            "   â„¹ï¸ Info Checkpoint - Epoch: 150, Loss: 0.0015605676775869877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at recobo/chemical-bert-uncased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Poids chargÃ©s avec succÃ¨s (strict=True)\n",
            "ğŸ“‚ Chargement des datasets...\n",
            "   Train: 31008 exemples\n",
            "   Val:   1000 exemples\n",
            "   ğŸ“š RÃ‰FÃ‰RENCE (train+val): 32008 exemples\n",
            "   Test:  1000 exemples (pour Kaggle)\n",
            "\n",
            "======================================================================\n",
            "ğŸ”„ ENCODAGE DE LA BANQUE DE RÃ‰FÃ‰RENCE (TRAIN+VAL)\n",
            "======================================================================\n",
            "ğŸ”„ Recalcul des embeddings de rÃ©fÃ©rence (32008 textes)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding Reference Texts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1001/1001 [01:45<00:00,  9.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ… Embeddings rÃ©fÃ©rence: torch.Size([32008, 768])\n",
            "\n",
            "======================================================================\n",
            "ğŸ¯ GÃ‰NÃ‰RATION SUBMISSION KAGGLE (TEST SET)\n",
            "======================================================================\n",
            "ğŸ§ª Encodage de 1000 graphes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding Graphs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 75.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Recherche des descriptions les plus proches (Test)...\n",
            "\n",
            "ğŸ’¾ Submission sauvegardÃ©e : submission_dual_encoder_final.csv\n",
            "   Nombre de prÃ©dictions : 1000\n",
            "   Preview :\n",
            " ID                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         description\n",
            "  0                                                                                                                                                                                                                                                                                                                         The molecule is a glycophytoceramide having an alpha-D-glucosyl residue at the O-1 position and a hexacosanoyl group attached to the nitrogen. It has a role as an antigen.\n",
            "  1 The molecule is the monohydrate form of doxapram hydrochloride. A central and respiratory stimulant with a brief duration of action, it is used as a temporary treatment of acute respiratory failure, particularly when superimposed on chronic obstructive pulmonary disease, and of postoperative respiratory depression. It has also been used for treatment of postoperative shivering. It has a role as a central nervous system stimulant. It contains a doxapram hydrochloride (anhydrous).\n",
            "  2                                                                              The molecule is a steroid glucosiduronic acid that is 5alpha-androstane-3beta,17beta-diol having a single beta-D-glucuronic acid residue attached at position 17. It is a steroid glucosiduronic acid, a beta-D-glucosiduronic acid and a 3beta-hydroxy steroid. It derives from a 5alpha-androstane-3beta,17beta-diol. It is a conjugate acid of a 5alpha-androstane-3beta,17beta-diol 17-O-(beta-D-glucuronide)(1-).\n",
            "  3                                                                                                                                                                                                                                                                                         The molecule is a hydroxy fatty acid ascaroside anion that is the conjugate base of oscr#29, obtained by deprotonation of the carboxy group; major species at pH 7.3. It is a conjugate base of an oscr#29.\n",
            "  4                                                                                                                                                                                                                                      The molecule is an organochlorine compound that consists of acetaldehyde where all the methyl hydrogens are replaced by chloro groups. It has a role as a mouse metabolite. It is an organochlorine compound and an aldehyde. It derives from an acetaldehyde.\n",
            "  5                                                                                                                                                                                      The molecule is an amino trisaccharide comprised of an N-acetylated glucosamine residue, sulfated on O-6, between two galactosyl residues. It is an intermediate in the keratan sulfate degradation pathway. It has a role as a mouse metabolite. It is an amino trisaccharide and an oligosaccharide sulfate.\n",
            "  6                                                                                                                                                                                                                                                                                                                                                            The molecule is a ribonic acid having L-configuration. It has a role as a bacterial metabolite. It is an enantiomer of a D-ribonic acid.\n",
            "  7                                                                                                                                                                  The molecule is a homodetic cyclic peptide that consists of L-valine as the amino acid residue. It is isolated from Lissoclinum bistratum and exhibits antitumour activity against the human colon tumour cell line. It has a role as a metabolite and an antineoplastic agent. It is a homodetic cyclic peptide and a macrocycle.\n",
            "  8                                                                                                                                                                                                                                                                                        The molecule is a monocarboxylic acid that is propanoic acid substituted by a 3,4-dimethoxyphenyl group at position 3. It is a monocarboxylic acid and a dimethoxybenzene. It derives from a propionic acid.\n",
            "  9                                                                                                                                                                                                                                                             The molecule is an organic cation resulting from the protonation of the amino group of validoxylamine A; major species at pH 7.3. It is an ammonium ion derivative and an organic cation. It is a conjugate acid of a validoxylamine A.\n",
            "\n",
            "ğŸ“ EXEMPLES DE PRÃ‰DICTIONS (TEST SET) :\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Id 0:\n",
            "  The molecule is a glycophytoceramide having an alpha-D-glucosyl residue at the O-1 position and a hexacosanoyl group att...\n",
            "\n",
            "Id 1:\n",
            "  The molecule is the monohydrate form of doxapram hydrochloride. A central and respiratory stimulant with a brief duratio...\n",
            "\n",
            "Id 2:\n",
            "  The molecule is a steroid glucosiduronic acid that is 5alpha-androstane-3beta,17beta-diol having a single beta-D-glucuro...\n",
            "\n",
            "Id 3:\n",
            "  The molecule is a hydroxy fatty acid ascaroside anion that is the conjugate base of oscr#29, obtained by deprotonation o...\n",
            "\n",
            "Id 4:\n",
            "  The molecule is an organochlorine compound that consists of acetaldehyde where all the methyl hydrogens are replaced by ...\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "======================================================================\n",
            "âœ… TERMINÃ‰ !\n",
            "======================================================================\n",
            "ğŸ“„ Fichier de submission : submission_dual_encoder_final.csv\n",
            "ğŸ“š Banque de rÃ©fÃ©rence   : 32008 descriptions\n",
            "ğŸ¯ PrÃ©dictions gÃ©nÃ©rÃ©es  : 1000\n",
            "======================================================================\n",
            "\n",
            "ğŸ’¡ Prochaine Ã©tape : Upload le fichier sur Kaggle !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_baseline/resume_training_dual_encoder.py\n",
        "import os\n",
        "import argparse\n",
        "import pickle\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "from torch_geometric.data import Batch\n",
        "from torch_geometric.nn import GPSConv, GINEConv, global_add_pool\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# --- DATASET ---\n",
        "class RawTextGraphDataset(Dataset):\n",
        "    def __init__(self, pkl_path):\n",
        "        self.pkl_path = Path(pkl_path)\n",
        "        with open(self.pkl_path, 'rb') as f:\n",
        "            self.data_list = pickle.load(f)\n",
        "    def __len__(self): return len(self.data_list)\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.data_list[idx]\n",
        "        text = data.description if hasattr(data, 'description') else \"\"\n",
        "        return data, text\n",
        "\n",
        "class DualCollate:\n",
        "    def __init__(self, tokenizer, max_len=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    def __call__(self, batch):\n",
        "        graphs, texts = zip(*batch)\n",
        "        batched_graphs = Batch.from_data_list(graphs)\n",
        "        text_inputs = self.tokenizer(list(texts), padding=True, truncation=True,\n",
        "                                     max_length=self.max_len, return_tensors=\"pt\")\n",
        "        return batched_graphs, text_inputs\n",
        "\n",
        "# --- MODELS ---\n",
        "ATOM_DIMS = [119, 4, 11, 12, 9, 5, 8, 2, 2]\n",
        "BOND_DIMS = [22, 6, 2]\n",
        "\n",
        "class AtomEncoder(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.ModuleList([nn.Embedding(dim, hidden_dim) for dim in ATOM_DIMS])\n",
        "    def forward(self, x):\n",
        "        return sum(emb(x[:, i]) for i, emb in enumerate(self.embeddings))\n",
        "\n",
        "class BondEncoder(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.ModuleList([nn.Embedding(dim, hidden_dim) for dim in BOND_DIMS])\n",
        "    def forward(self, edge_attr):\n",
        "        return sum(emb(edge_attr[:, i]) for i, emb in enumerate(self.embeddings))\n",
        "\n",
        "class MolGNN(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, out_dim=768, num_layers=4, num_heads=8, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.atom_encoder = AtomEncoder(hidden_dim)\n",
        "        self.bond_encoder = BondEncoder(hidden_dim)\n",
        "        self.convs = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            local_nn = nn.Sequential(\n",
        "                nn.Linear(hidden_dim, 2 * hidden_dim),\n",
        "                nn.BatchNorm1d(2 * hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(2 * hidden_dim, hidden_dim),\n",
        "            )\n",
        "            self.convs.append(GPSConv(\n",
        "                hidden_dim,\n",
        "                GINEConv(local_nn, train_eps=True, edge_dim=hidden_dim),\n",
        "                heads=num_heads,\n",
        "                dropout=dropout,\n",
        "                attn_type='multihead'\n",
        "            ))\n",
        "        self.pool = global_add_pool\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, batch):\n",
        "        h = self.atom_encoder(batch.x)\n",
        "        edge_attr = self.bond_encoder(batch.edge_attr)\n",
        "        for conv in self.convs:\n",
        "            h = conv(h, batch.edge_index, batch.batch, edge_attr=edge_attr)\n",
        "        return self.proj(self.pool(h, batch.batch))\n",
        "\n",
        "class DualEncoder(nn.Module):\n",
        "    def __init__(self, model_name, gnn_args, freeze_layers=0):\n",
        "        super().__init__()\n",
        "        self.graph_encoder = MolGNN(**gnn_args)\n",
        "        self.text_encoder = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        if freeze_layers > 0:\n",
        "            print(f\"â„ï¸ Gel des {freeze_layers} premiÃ¨res couches de BERT\")\n",
        "            for param in self.text_encoder.embeddings.parameters():\n",
        "                param.requires_grad = False\n",
        "            for i in range(freeze_layers):\n",
        "                if i < len(self.text_encoder.encoder.layer):\n",
        "                    for param in self.text_encoder.encoder.layer[i].parameters():\n",
        "                        param.requires_grad = False\n",
        "\n",
        "        bert_dim = self.text_encoder.config.hidden_size\n",
        "        out_dim = gnn_args['out_dim']\n",
        "        self.text_proj = nn.Linear(bert_dim, out_dim) if bert_dim != out_dim else nn.Identity()\n",
        "\n",
        "    def forward(self, batch_graphs, text_inputs):\n",
        "        g_emb = self.graph_encoder(batch_graphs)\n",
        "        t_out = self.text_encoder(**text_inputs)\n",
        "        t_emb = self.text_proj(t_out.last_hidden_state[:, 0, :])\n",
        "        return F.normalize(g_emb, dim=-1), F.normalize(t_emb, dim=-1)\n",
        "\n",
        "# --- LOSS ---\n",
        "def triplet_loss(mol_vec, txt_vec, margin=0.2):\n",
        "    sims = mol_vec @ txt_vec.t()\n",
        "    mask = torch.eye(sims.size(0), device=sims.device).bool()\n",
        "    min_value = torch.finfo(sims.dtype).min\n",
        "    neg_sims = sims.masked_fill(mask, min_value)\n",
        "\n",
        "    hard_neg_m2t = neg_sims.max(dim=1, keepdim=True)[0]\n",
        "    hard_neg_t2m = neg_sims.max(dim=0, keepdim=True)[0].t()\n",
        "    pos_sims = sims.diag().unsqueeze(1)\n",
        "\n",
        "    loss = (F.relu(margin + hard_neg_m2t - pos_sims).mean() +\n",
        "            F.relu(margin + hard_neg_t2m - pos_sims).mean())\n",
        "    return loss / 2\n",
        "\n",
        "# --- BACKUP FONCTION ---\n",
        "def backup_to_drive(file_path, drive_backup_dir):\n",
        "    \"\"\"Copie automatique d'un fichier vers Google Drive\"\"\"\n",
        "    if not Path('/content/drive/MyDrive').exists():\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        drive_dir = Path(drive_backup_dir)\n",
        "        drive_dir.mkdir(parents=True, exist_ok=True)\n",
        "        dest_path = drive_dir / Path(file_path).name\n",
        "        shutil.copy2(file_path, dest_path)\n",
        "        size_mb = Path(file_path).stat().st_size / (1024**2)\n",
        "        print(f\"  â˜ï¸ Backup Drive: {Path(file_path).name} ({size_mb:.1f} MB)\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âš ï¸ Backup Drive Ã©chouÃ©: {e}\")\n",
        "\n",
        "# --- MAIN ---\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data_dir', type=str, default='data_baseline/data')\n",
        "    parser.add_argument('--model_name', type=str, default='recobo/chemical-bert-uncased')\n",
        "    parser.add_argument('--drive_backup_dir', type=str, default='/content/drive/MyDrive/dual_encoder_checkpoints')\n",
        "\n",
        "    #  REPRISE D'ENTRAÃNEMENT\n",
        "    parser.add_argument('--resume_from', type=str, required=True,\n",
        "                       help='Chemin vers le checkpoint Ã  reprendre (ex: dual_FINAL_..._ep150.pt)')\n",
        "    parser.add_argument('--total_epochs', type=int, default=250,\n",
        "                       help='Nombre total d\\'epochs voulu (ex: 250 pour continuer de 150 Ã  250)')\n",
        "\n",
        "    parser.add_argument('--lr_gnn', type=float, default=8e-4)\n",
        "    parser.add_argument('--lr_bert', type=float, default=3e-5)\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-4)\n",
        "    parser.add_argument('--freeze_layers', type=int, default=0)\n",
        "    parser.add_argument('--margin', type=float, default=0.2)\n",
        "    parser.add_argument('--batch_size', type=int, default=16)\n",
        "    parser.add_argument('--grad_accum', type=int, default=8)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # === 1. CHARGEMENT DU CHECKPOINT ===\n",
        "    print(\"=\" * 70)\n",
        "    print(\"ğŸ”„ REPRISE D'ENTRAÃNEMENT - DUAL ENCODER (8 heads)\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"ğŸ“¥ Chargement du checkpoint : {args.resume_from}\")\n",
        "\n",
        "    checkpoint = torch.load(args.resume_from, map_location=DEVICE)\n",
        "\n",
        "    # Extraire les infos du checkpoint\n",
        "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "        state_dict = checkpoint['model_state_dict']\n",
        "        saved_args = checkpoint.get('args', {})\n",
        "        start_epoch = checkpoint.get('epoch', 0)\n",
        "\n",
        "        # Utiliser les hyperparamÃ¨tres sauvegardÃ©s\n",
        "        args.lr_gnn = saved_args.get('lr_gnn', args.lr_gnn)\n",
        "        args.lr_bert = saved_args.get('lr_bert', args.lr_bert)\n",
        "        args.weight_decay = saved_args.get('weight_decay', args.weight_decay)\n",
        "        args.freeze_layers = saved_args.get('freeze_layers', args.freeze_layers)\n",
        "        args.margin = saved_args.get('margin', args.margin)\n",
        "        args.batch_size = saved_args.get('batch_size', args.batch_size)\n",
        "        args.grad_accum = saved_args.get('grad_accum', args.grad_accum)\n",
        "\n",
        "        print(f\"âœ… Checkpoint chargÃ© : Epoch {start_epoch}\")\n",
        "        print(f\"   Loss: {checkpoint.get('train_loss', 'N/A')}\")\n",
        "    else:\n",
        "        raise ValueError(\"Format de checkpoint non reconnu\")\n",
        "\n",
        "    drive_available = Path('/content/drive/MyDrive').exists()\n",
        "\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"Device      : {DEVICE}\")\n",
        "    if drive_available:\n",
        "        print(f\"â˜ï¸ Google Drive: MONTÃ‰\")\n",
        "    print(f\"LR GNN      : {args.lr_gnn}\")\n",
        "    print(f\"LR BERT     : {args.lr_bert}\")\n",
        "    print(f\"Weight Decay: {args.weight_decay}\")\n",
        "    print(f\"Freeze Layers: {args.freeze_layers}\")\n",
        "    print(f\"Margin      : {args.margin}\")\n",
        "    print(f\"Batch Size  : {args.batch_size} Ã— {args.grad_accum} = {args.batch_size * args.grad_accum}\")\n",
        "    print(f\"ğŸ“ˆ Reprise   : Epoch {start_epoch} â†’ {args.total_epochs}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
        "\n",
        "    # === 2. DATASET (TRAIN+VAL) ===\n",
        "    train_dataset = RawTextGraphDataset(Path(args.data_dir) / \"train_graphs.pkl\")\n",
        "    val_dataset = RawTextGraphDataset(Path(args.data_dir) / \"validation_graphs.pkl\")\n",
        "    combined_dataset = ConcatDataset([train_dataset, val_dataset])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        combined_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=DualCollate(tokenizer),\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\"ğŸ“Š Training sur {len(combined_dataset)} exemples (train: {len(train_dataset)}, val: {len(val_dataset)})\")\n",
        "\n",
        "    # === 3. MODÃˆLE (8 HEADS) ===\n",
        "    gnn_config = {\n",
        "        'hidden_dim': 256,\n",
        "        'out_dim': 768,\n",
        "        'num_layers': 4,\n",
        "        'num_heads': 8,  # â† 8 heads (comme le checkpoint)\n",
        "        'dropout': 0.1\n",
        "    }\n",
        "\n",
        "    model = DualEncoder(args.model_name, gnn_config, freeze_layers=args.freeze_layers).to(DEVICE)\n",
        "\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "    print(\"âœ… Poids du modÃ¨le chargÃ©s\")\n",
        "\n",
        "    # === 4. OPTIMIZER & SCHEDULER ===\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': model.graph_encoder.parameters(),\n",
        "         'lr': args.lr_gnn,\n",
        "         'weight_decay': args.weight_decay},\n",
        "        {'params': model.text_encoder.parameters(),\n",
        "         'lr': args.lr_bert,\n",
        "         'weight_decay': args.weight_decay / 10},\n",
        "        {'params': model.text_proj.parameters(),\n",
        "         'lr': args.lr_bert,\n",
        "         'weight_decay': args.weight_decay}\n",
        "    ])\n",
        "\n",
        "    #  NOUVEAU SCHEDULER pour la continuation\n",
        "    # On crÃ©e un scheduler qui part de start_epoch\n",
        "    remaining_epochs = args.total_epochs - start_epoch\n",
        "\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=[args.lr_gnn, args.lr_bert, args.lr_bert],\n",
        "        steps_per_epoch=len(train_loader) // args.grad_accum,\n",
        "        epochs=remaining_epochs,  # Epochs restants\n",
        "        pct_start=0.05,  # Petit warmup pour reprendre doucement\n",
        "        anneal_strategy='cos'\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… Scheduler configurÃ© pour {remaining_epochs} epochs supplÃ©mentaires\")\n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    log_file = Path(args.data_dir) / \"training_log_final_resumed.json\"\n",
        "    if log_file.exists():\n",
        "        with open(log_file, 'r') as f:\n",
        "            training_logs = json.load(f)\n",
        "        print(f\"ğŸ“ Logs existants chargÃ©s ({len(training_logs)} epochs)\")\n",
        "    else:\n",
        "        training_logs = []\n",
        "\n",
        "    print(\"\\nğŸ‹ï¸ Reprise de l'entraÃ®nement...\\n\")\n",
        "\n",
        "    # === 5. BOUCLE D'ENTRAÃNEMENT ===\n",
        "    for epoch in range(start_epoch, args.total_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for step, (graphs, text_inputs) in enumerate(train_loader):\n",
        "            graphs = graphs.to(DEVICE)\n",
        "            text_inputs = {k: v.to(DEVICE) for k, v in text_inputs.items()}\n",
        "\n",
        "            with autocast():\n",
        "                g_emb, t_emb = model(graphs, text_inputs)\n",
        "                loss = triplet_loss(g_emb, t_emb, margin=args.margin) / args.grad_accum\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (step + 1) % args.grad_accum == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "\n",
        "            total_loss += loss.item() * args.grad_accum\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Log\n",
        "        log_entry = {\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': avg_loss\n",
        "        }\n",
        "        training_logs.append(log_entry)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{args.total_epochs} | Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % 5 == 0 or epoch == args.total_epochs - 1:\n",
        "            effective_bs = args.batch_size * args.grad_accum\n",
        "            save_name = (f\"dual_FINAL_RESUMED_lrGNN{args.lr_gnn}_lrBERT{args.lr_bert}_\"\n",
        "                        f\"wd{args.weight_decay}_frz{args.freeze_layers}_\"\n",
        "                        f\"margin{args.margin}_bs{effective_bs}_ep{epoch+1}.pt\")\n",
        "\n",
        "            checkpoint_path = Path(args.data_dir) / save_name\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'args': vars(args),\n",
        "                'epoch': epoch + 1,\n",
        "                'train_loss': avg_loss\n",
        "            }, checkpoint_path)\n",
        "\n",
        "            print(f\"  ğŸ’¾ Checkpoint sauvegardÃ© : {save_name}\")\n",
        "\n",
        "            if drive_available:\n",
        "                backup_to_drive(checkpoint_path, args.drive_backup_dir)\n",
        "\n",
        "        with open(log_file, 'w') as f:\n",
        "            json.dump(training_logs, f, indent=2)\n",
        "\n",
        "        if drive_available and ((epoch + 1) % 10 == 0 or epoch == args.total_epochs - 1):\n",
        "            backup_to_drive(log_file, args.drive_backup_dir)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"âœ… EntraÃ®nement terminÃ© !\")\n",
        "    print(f\"ğŸ“ Logs sauvegardÃ©s dans : {log_file}\")\n",
        "    if drive_available:\n",
        "        print(f\"â˜ï¸ Backups disponibles sur : {args.drive_backup_dir}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtlFQHlOoZ4G",
        "outputId": "cf64f3ae-f5d9-44dd-99dd-378f783c4790"
      },
      "id": "QtlFQHlOoZ4G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_baseline/resume_training_dual_encoder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python data_baseline/resume_training_dual_encoder.py \\\n",
        "    --resume_from data_baseline/data/dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep150.pt \\\n",
        "    --total_epochs 250 \\\n",
        "    --data_dir data_baseline/data \\\n",
        "    --drive_backup_dir /content/drive/MyDrive/Altegrad_Results/dual_encoder_checkpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn7WUKlaoqvs",
        "outputId": "fb76242c-2c11-471e-ea89-80b243615706"
      },
      "id": "nn7WUKlaoqvs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ğŸ”„ REPRISE D'ENTRAÃNEMENT - DUAL ENCODER (8 heads)\n",
            "======================================================================\n",
            "ğŸ“¥ Chargement du checkpoint : data_baseline/data/dual_FINAL_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep150.pt\n",
            "âœ… Checkpoint chargÃ© : Epoch 150\n",
            "   Loss: 0.0013596335987279798\n",
            "----------------------------------------------------------------------\n",
            "Device      : cuda\n",
            "â˜ï¸ Google Drive: MONTÃ‰\n",
            "LR GNN      : 0.0008\n",
            "LR BERT     : 3e-05\n",
            "Weight Decay: 0.0001\n",
            "Freeze Layers: 0\n",
            "Margin      : 0.2\n",
            "Batch Size  : 16 Ã— 8 = 128\n",
            "ğŸ“ˆ Reprise   : Epoch 150 â†’ 250\n",
            "======================================================================\n",
            "ğŸ“Š Training sur 32008 exemples (train: 31008, val: 1000)\n",
            "2026-01-12 19:17:40.277973: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768245460.304316  127352 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768245460.311584  127352 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768245460.330012  127352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768245460.330036  127352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768245460.330040  127352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768245460.330043  127352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Some weights of BertModel were not initialized from the model checkpoint at recobo/chemical-bert-uncased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "âœ… Poids du modÃ¨le chargÃ©s\n",
            "âœ… Scheduler configurÃ© pour 100 epochs supplÃ©mentaires\n",
            "/content/BornToOverfit/data_baseline/resume_training_dual_encoder.py:281: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "\n",
            "ğŸ‹ï¸ Reprise de l'entraÃ®nement...\n",
            "\n",
            "/content/BornToOverfit/data_baseline/resume_training_dual_encoder.py:304: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Epoch 151/250 | Train Loss: 0.0014\n",
            "Epoch 152/250 | Train Loss: 0.0016\n",
            "Epoch 153/250 | Train Loss: 0.0020\n",
            "Epoch 154/250 | Train Loss: 0.0028\n",
            "Epoch 155/250 | Train Loss: 0.0035\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep155.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep155.pt (433.8 MB)\n",
            "Epoch 156/250 | Train Loss: 0.0037\n",
            "Epoch 157/250 | Train Loss: 0.0036\n",
            "Epoch 158/250 | Train Loss: 0.0034\n",
            "Epoch 159/250 | Train Loss: 0.0037\n",
            "Epoch 160/250 | Train Loss: 0.0036\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep160.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep160.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final_resumed.json (0.0 MB)\n",
            "Epoch 161/250 | Train Loss: 0.0038\n",
            "Epoch 162/250 | Train Loss: 0.0035\n",
            "Epoch 163/250 | Train Loss: 0.0033\n",
            "Epoch 164/250 | Train Loss: 0.0032\n",
            "Epoch 165/250 | Train Loss: 0.0034\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep165.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep165.pt (433.8 MB)\n",
            "Epoch 166/250 | Train Loss: 0.0032\n",
            "Epoch 167/250 | Train Loss: 0.0030\n",
            "Epoch 168/250 | Train Loss: 0.0037\n",
            "Epoch 169/250 | Train Loss: 0.0036\n",
            "Epoch 170/250 | Train Loss: 0.0034\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep170.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep170.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final_resumed.json (0.0 MB)\n",
            "Epoch 171/250 | Train Loss: 0.0031\n",
            "Epoch 172/250 | Train Loss: 0.0029\n",
            "Epoch 173/250 | Train Loss: 0.0031\n",
            "Epoch 174/250 | Train Loss: 0.0031\n",
            "Epoch 175/250 | Train Loss: 0.0031\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep175.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep175.pt (433.8 MB)\n",
            "Epoch 176/250 | Train Loss: 0.0031\n",
            "Epoch 177/250 | Train Loss: 0.0028\n",
            "Epoch 178/250 | Train Loss: 0.0027\n",
            "Epoch 179/250 | Train Loss: 0.0026\n",
            "Epoch 180/250 | Train Loss: 0.0027\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep180.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep180.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final_resumed.json (0.0 MB)\n",
            "Epoch 181/250 | Train Loss: 0.0026\n",
            "Epoch 182/250 | Train Loss: 0.0027\n",
            "Epoch 183/250 | Train Loss: 0.0027\n",
            "Epoch 184/250 | Train Loss: 0.0023\n",
            "Epoch 185/250 | Train Loss: 0.0025\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep185.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep185.pt (433.8 MB)\n",
            "Epoch 186/250 | Train Loss: 0.0025\n",
            "Epoch 187/250 | Train Loss: 0.0025\n",
            "Epoch 188/250 | Train Loss: 0.0025\n",
            "Epoch 189/250 | Train Loss: 0.0025\n",
            "Epoch 190/250 | Train Loss: 0.0024\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep190.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep190.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final_resumed.json (0.0 MB)\n",
            "Epoch 191/250 | Train Loss: 0.0025\n",
            "Epoch 192/250 | Train Loss: 0.0024\n",
            "Epoch 193/250 | Train Loss: 0.0024\n",
            "Epoch 194/250 | Train Loss: 0.0025\n",
            "Epoch 195/250 | Train Loss: 0.0026\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep195.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep195.pt (433.8 MB)\n",
            "Epoch 196/250 | Train Loss: 0.0023\n",
            "Epoch 197/250 | Train Loss: 0.0023\n",
            "Epoch 198/250 | Train Loss: 0.0021\n",
            "Epoch 199/250 | Train Loss: 0.0022\n",
            "Epoch 200/250 | Train Loss: 0.0020\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep200.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep200.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final_resumed.json (0.0 MB)\n",
            "Epoch 201/250 | Train Loss: 0.0018\n",
            "Epoch 202/250 | Train Loss: 0.0018\n",
            "Epoch 203/250 | Train Loss: 0.0018\n",
            "Epoch 204/250 | Train Loss: 0.0017\n",
            "Epoch 205/250 | Train Loss: 0.0018\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep205.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep205.pt (433.8 MB)\n",
            "Epoch 206/250 | Train Loss: 0.0018\n",
            "Epoch 207/250 | Train Loss: 0.0019\n",
            "Epoch 208/250 | Train Loss: 0.0021\n",
            "Epoch 209/250 | Train Loss: 0.0016\n",
            "Epoch 210/250 | Train Loss: 0.0017\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep210.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep210.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final_resumed.json (0.0 MB)\n",
            "Epoch 211/250 | Train Loss: 0.0017\n",
            "Epoch 212/250 | Train Loss: 0.0017\n",
            "Epoch 213/250 | Train Loss: 0.0017\n",
            "Epoch 214/250 | Train Loss: 0.0016\n",
            "Epoch 215/250 | Train Loss: 0.0018\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep215.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep215.pt (433.8 MB)\n",
            "Epoch 216/250 | Train Loss: 0.0015\n",
            "Epoch 217/250 | Train Loss: 0.0015\n",
            "Epoch 218/250 | Train Loss: 0.0015\n",
            "Epoch 219/250 | Train Loss: 0.0014\n",
            "Epoch 220/250 | Train Loss: 0.0013\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep220.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep220.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final_resumed.json (0.0 MB)\n",
            "Epoch 221/250 | Train Loss: 0.0012\n",
            "Epoch 222/250 | Train Loss: 0.0014\n",
            "Epoch 223/250 | Train Loss: 0.0015\n",
            "Epoch 224/250 | Train Loss: 0.0013\n",
            "Epoch 225/250 | Train Loss: 0.0015\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep225.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep225.pt (433.8 MB)\n",
            "Epoch 226/250 | Train Loss: 0.0014\n",
            "Epoch 227/250 | Train Loss: 0.0014\n",
            "Epoch 228/250 | Train Loss: 0.0012\n",
            "Epoch 229/250 | Train Loss: 0.0012\n",
            "Epoch 230/250 | Train Loss: 0.0013\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep230.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep230.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final_resumed.json (0.0 MB)\n",
            "Epoch 231/250 | Train Loss: 0.0014\n",
            "Epoch 232/250 | Train Loss: 0.0013\n",
            "Epoch 233/250 | Train Loss: 0.0014\n",
            "Epoch 234/250 | Train Loss: 0.0012\n",
            "Epoch 235/250 | Train Loss: 0.0013\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep235.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep235.pt (433.8 MB)\n",
            "Epoch 236/250 | Train Loss: 0.0013\n",
            "Epoch 237/250 | Train Loss: 0.0013\n",
            "Epoch 238/250 | Train Loss: 0.0013\n",
            "Epoch 239/250 | Train Loss: 0.0012\n",
            "Epoch 240/250 | Train Loss: 0.0014\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep240.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep240.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final_resumed.json (0.0 MB)\n",
            "Epoch 241/250 | Train Loss: 0.0013\n",
            "Epoch 242/250 | Train Loss: 0.0013\n",
            "Epoch 243/250 | Train Loss: 0.0013\n",
            "Epoch 244/250 | Train Loss: 0.0012\n",
            "Epoch 245/250 | Train Loss: 0.0013\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep245.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep245.pt (433.8 MB)\n",
            "Epoch 246/250 | Train Loss: 0.0013\n",
            "Epoch 247/250 | Train Loss: 0.0012\n",
            "Epoch 248/250 | Train Loss: 0.0013\n",
            "Epoch 249/250 | Train Loss: 0.0012\n",
            "Epoch 250/250 | Train Loss: 0.0011\n",
            "  ğŸ’¾ Checkpoint sauvegardÃ© : dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep250.pt\n",
            "  â˜ï¸ Backup Drive: dual_FINAL_RESUMED_lrGNN0.0008_lrBERT3e-05_wd0.0001_frz0_margin0.2_bs128_ep250.pt (433.8 MB)\n",
            "  â˜ï¸ Backup Drive: training_log_final_resumed.json (0.0 MB)\n",
            "\n",
            "======================================================================\n",
            "âœ… EntraÃ®nement terminÃ© !\n",
            "ğŸ“ Logs sauvegardÃ©s dans : data_baseline/data/training_log_final_resumed.json\n",
            "â˜ï¸ Backups disponibles sur : /content/drive/MyDrive/Altegrad_Results/dual_encoder_checkpoints\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____"
      ],
      "metadata": {
        "id": "iTGmXkh7_wIa"
      },
      "id": "iTGmXkh7_wIa"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "collapsed_sections": [
        "9zO0bwAMOR9v",
        "5a92752f",
        "c2B-4AyZTpE7",
        "SjBS7_L6YHjw",
        "K-vUdCMMhAsx"
      ],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}